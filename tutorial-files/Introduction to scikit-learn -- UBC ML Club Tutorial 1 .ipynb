{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "* Implement a simple machine learning classifier with **Python** and **scikit-learn**\n",
    "\n",
    "* Learn a *straightforward and reusable template* for machine learning\n",
    "\n",
    "* Complete a **fun machine learning project!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This tutorial is **very interactive** so to follow along, please make sure you have *numpy*, *pandas* and *scikit-learn* installed. \n",
    "\n",
    "If you don't have these installed, you can get these packages (and many more useful packages for machine learning) by downloading **Anaconda**. You can do this by visiting https://www.anaconda.com/download/.\n",
    "\n",
    "Once you're done the installation, the following packages should import succesfully in a Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "There's *many* different types of machine learning problems, but the one that we're going to focus on in this tutorial is **classification**.\n",
    "\n",
    "In a classification problem, we are given observations/details about an object and we want to **assign a category or class** to the object.\n",
    "\n",
    "* Length and width of Sepal/Petal --> Type of flower\n",
    "* Image of digit --> Digit\n",
    "* Information about a passenger on the titanic --> Whether or not the passenger survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "The problem that we're going to explore in this tutorial is that of predicting types of irises. For this purpose, we use the **Iris** dataset.\n",
    "\n",
    "The Iris dataset consists of 3 different types of irises (*Setosa, Versicolour, and Virginica*), which we must predict given their petal/sepal length/width. If, like me, you have no idea what the difference between a sepal and a petal is, the image below should help!\n",
    "\n",
    "![test](https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Petal-sepal.jpg/220px-Petal-sepal.jpg)\n",
    "\n",
    "The code below loads this data from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"X:\", X)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Generally, we want to **train** our classifiers and then **evaluate** their performance. To do this, we need to split our dataset.\n",
    "\n",
    "The larger the size of your training set, the more data your model has to learn from. The larger the size of your testing set, the more confident you can be in your evaluation. For our purposes, we will do an 80/20 split with 80% of the data being used for training and the remaining 20% being used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n",
    "\n",
    "scikit-learn implements a large numbers of commonly used machine learning models. Some commonly used ones are listed below:\n",
    "\n",
    "* Support Vector Machine (*sklearn.svm.SVC*)\n",
    "* Logistic Regression (*sklearn.linear_model.LogisticRegression*)\n",
    "* Random Forest (*sklearn.ensemble.RandomForestClassifier*)\n",
    "* Multi-layer Perceptron (*sklearn..neural_network.MLPClassifier*)random forest random forest \n",
    "\n",
    "We're going to use a **random forest** consisting of 20 decision trees. A **decision tree** essentially consists of a sequence of questions pertaining to the features, that ultimately generate a decision (sort of like a flow chart).\n",
    "\n",
    "![Example of a decision tree](http://dataaspirant.com/wp-content/uploads/2017/01/B03905_05_01-compressor.png)\n",
    "\n",
    "A **random forest** trains multiple decision trees with *different subsets of the features* and averages out the predicions of these trees during prediction time. The intuition behind random forests is that the decision trees learn to identify *different* patterns and relationships in the data.\n",
    "\n",
    "![Example of a random forest](https://d2wh20haedxe3f.cloudfront.net/sites/default/files/random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define and train the random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction on the first testing sample\n",
    "print(\"Testing sample:\", X_test[0])\n",
    "print(\"Prediction:\", clf.predict([X_test[0]]))\n",
    "print(\"Ground truth:\", y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We now need to evaluate the quality of our model. In order to do this, we use a couple of metrics that are commonly used for classification problems.\n",
    "\n",
    "![Metric definition](https://i.stack.imgur.com/z5WJHm.jpg)\n",
    "\n",
    "**Precision**: *how often are we correct when predicting a particular label*\n",
    "\n",
    "**Recall**: *how often do we get all of the samples for a particular label*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on all of our testing samples\n",
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our predictions\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Great work! You have now trained a model to predict iris-types, and you'll never go another day without knowing the type of iris you're looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!\n",
    "\n",
    "Now it's your turn to train a machine learning model to predict who survives on the titanic! Visit http://ubcml.com/challenges/titanic to download the starter code.\n",
    "\n",
    "The starter file already goes through the process of training a model and dumping the generated predictions to a file for you. Nonetheless, we'll go through some data processing and visualization to get you familiarized with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', header=0) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(['Survived'], axis=1)\n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df[['Survived']]\n",
    "df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_X[['Pclass', 'Sex', 'Age', 'Fare']]\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_features.as_matrix()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:,1] = [int(e == \"male\") for e in features[:,1]]\n",
    "X = np.nan_to_num(features.astype(\"float\"))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_Y.as_matrix()[:,0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
